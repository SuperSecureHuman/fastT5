{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastT5 import  generate_onnx_representation\n",
    "\n",
    "\n",
    "model_or_model_path = 't5-base'\n",
    "\n",
    "# Step 1. convert huggingfaces t5 model to onnx\n",
    "onnx_model_paths = generate_onnx_representation(model_or_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastT5 import quantize\n",
    "\n",
    "# Step 2. (recommended) quantize the converted model for fast inference and to reduce model size.\n",
    "quant_model_paths = quantize(onnx_model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model_paths = ('/home/venom/repo/fastT5/models/t5-base-encoder-quantized.onnx',\n",
    " '/home/venom/repo/fastT5/models/t5-base-decoder-quantized.onnx',\n",
    " '/home/venom/repo/fastT5/models/t5-base-init-decoder-quantized.onnx')\n",
    "\n",
    "model_or_model_path = 't5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 11:48:56.828668139 [W:onnxruntime:, session_state.cc:1169 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-09-18 11:48:56.828694993 [W:onnxruntime:, session_state.cc:1171 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n",
      "2023-09-18 11:48:57.460451364 [W:onnxruntime:, session_state.cc:1169 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-09-18 11:48:57.460485963 [W:onnxruntime:, session_state.cc:1171 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    }
   ],
   "source": [
    "from fastT5 import get_onnx_runtime_sessions, OnnxT5\n",
    "\n",
    "# step 3. setup onnx runtime\n",
    "model_sessions = get_onnx_runtime_sessions(quant_model_paths, provider=[\"OpenVINOExecutionProvider\"])\n",
    "\n",
    "# step 4. get the onnx model\n",
    "model = OnnxT5(model_or_model_path, model_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venom/mambaforge/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Translate English to French: Lisa Marie Nowak (nÃ©e Caputo, born May 10, 1963) is an American aeronautical engineer, convicted criminal, and former NASA astronaut and United States Navy officer. Nowak served as naval flight officer and test pilot in the Navy, and was selected by NASA for NASA Astronaut Group 16 in 1996, qualifying as a mission specialist in robotics. She flew in space aboard Space Shuttle Discovery during the STS-121 mission in July 2006, when she was responsible for operating the robotic arms of the shuttle and the International Space Station. In 2007, Nowak was involved in a highly publicized incident of criminal misconduct for which she eventually pled guilty to felony burglary and misdemeanor battery charges, resulting in her demotion from captain to commander, and termination by NASA and the Navy. Born in Washington, D.C., Nowak graduated from the United States Naval Academy in Annapolis, Maryland, in 1985. She was assigned to VAQ-34 at Naval Air Station Point Mugu, California, where she flew the EA-7L Corsair and ERA-3B Skywarrior. She earned a Master of Science degree in aeronautical engineering and a degree in aeronautical and astronautical engineering from the Naval Postgraduate School in Monterey, California. In 1993 she was selected to attend the U.S. Naval Test Pilot School at Naval Air Station Patuxent River, Maryland. After graduation, she remained at Patuxent River, flying in the F/A-18 Hornet and EA-6B Prowler. During her Navy career she logged over 1,500 hours in more than 30 aircraft and was awarded the Defense Meritorious Service Medal, the Navy Commendation Medal and the Navy Achievement Medal.In February 2007, Nowak was arrested in Orlando, Florida, after she accosted and pepper-sprayed Colleen Shipman, a U.S. Air Force captain romantically involved with astronaut William Oefelein, who had been in a relationship with Nowak. She was released on bail and initially pleaded not guilty to the charges, which included attempted kidnapping, burglary with assault, and battery. Subsequently, her assignment as an astronaut was terminated by NASA. In 2009, Nowak agreed to a plea deal with prosecutors and pleaded guilty to charges of felony burglary of a car and misdemeanor battery. She remained a Navy captain until the following year when a Naval Board of Inquiry voted unanimously to reduce her in rank to commander and to discharge her from the Navy under other than honorable conditions after 25 years of service. As of 2017, it was reported that she was working in the private sector in Texas. Early life and education Lisa Marie Caputo was born in Washington, D.C., on May 10, 1963, to Alfredo F. Caputo, a computer consultant, and Jane L.and termination by NASA and the Navy. Born in Washington, D.C., Nowak graduated from the United States Naval Academy in Annapolis, Maryland, in 1985. She was assigned to VAQ-34 at Naval Air Station Point Mugu, California, where she flew the EA-7L Corsair and ERA-3B Skywarrior. She earned a Master of Science degree in aeronautical engineering and a degree in aeronautical and astronautical engineering from the Naval Postgraduate School in Monterey, California. In 1993 she was selected to attend the U.S. Naval Test Pilot School at Naval Air Station Patuxent River, Maryland. After graduation, she remained at Patuxent River, flying in the F/A-18 Hornet and EA-6B Prowler. During her Navy career she logged over 1,500 hours in more than 30 aircraft and was awarded the Defense Meritorious Service Medal, the Navy Commendation Medal and the Navy Achievement Medal. In February 2007, Nowak was arrested in Orlando, Florida, after she accosted and pepper-sprayed Colleen Shipman, a U.S. Air Force captain romantically involved with astronaut William Oefelein, who had been in a relationship with Nowak. She was released on bail and initially pleaded not guilty to the charges, which included attempted kidnapping, burglary with assault, and battery. Subsequently, her assignment as an astronaut was terminated by NASA. In 2009, Nowak agreed to a plea deal with prosecutors and pleaded guilty to charges of felony burglary of a car and misdemeanor battery. She remained a Navy captain until the following year when a Naval Board of Inquiry voted unanimously to reduce her in rank to commander and to discharge her from the Navy under other than honorable conditions after 25 years of service. As of 2017, it was reported that she was working in the private sector in Texas. Early life and education Lisa Marie Caputo was born in Washington, D.C., on May 10, 1963, to Alfredo F. Caputo, a computer consultant, and Jane L\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = tokenizer(text, return_tensors='pt', max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venom/mambaforge/lib/python3.9/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeException",
     "evalue": "[ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Encountered unknown exception in Run()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/venom/repo/fastT5/sample.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/venom/repo/fastT5/sample.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49mtoken[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/venom/repo/fastT5/sample.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                attention_mask\u001b[39m=\u001b[39;49mtoken[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m],)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/transformers/generation/utils.py:1522\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1517\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1518\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m         )\n\u001b[1;32m   1521\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1522\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1523\u001b[0m         input_ids,\n\u001b[1;32m   1524\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1525\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1526\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1527\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1528\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1529\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1530\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1531\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1532\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1533\u001b[0m     )\n\u001b[1;32m   1535\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1536\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/transformers/generation/utils.py:2339\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2336\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2338\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2339\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2340\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2341\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2342\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2343\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2344\u001b[0m )\n\u001b[1;32m   2346\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2347\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repo/fastT5/fastT5/onnx_models.py:182\u001b[0m, in \u001b[0;36mOnnxT5.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    177\u001b[0m         decoder_inputs_embeds \u001b[39m=\u001b[39m decoder_inputs_embeds[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m     \u001b[39m# runs only for the first time:\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     init_onnx_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder_init(\n\u001b[1;32m    183\u001b[0m         decoder_input_ids, attention_mask, encoder_hidden_states\n\u001b[1;32m    184\u001b[0m     )\n\u001b[1;32m    186\u001b[0m     logits, past_key_values \u001b[39m=\u001b[39m init_onnx_outputs\n\u001b[1;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repo/fastT5/fastT5/onnx_models.py:62\u001b[0m, in \u001b[0;36mT5DecoderInit.forward\u001b[0;34m(self, input_ids, encoder_attention_mask, encoder_hidden_states)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, encoder_attention_mask, encoder_hidden_states):\n\u001b[0;32m---> 62\u001b[0m     decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     63\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     64\u001b[0m         {\n\u001b[1;32m     65\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m: input_ids\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(),\n\u001b[1;32m     66\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mencoder_attention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m: encoder_attention_mask\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(),\n\u001b[1;32m     67\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mencoder_hidden_states\u001b[39;49m\u001b[39m\"\u001b[39;49m: encoder_hidden_states\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy(),\n\u001b[1;32m     68\u001b[0m         },\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     71\u001b[0m     list_pkv \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(torch\u001b[39m.\u001b[39mfrom_numpy(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m decoder_outputs[\u001b[39m1\u001b[39m:])\n\u001b[1;32m     73\u001b[0m     out_past_key_values \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\n\u001b[1;32m     74\u001b[0m         list_pkv[i : i \u001b[39m+\u001b[39m \u001b[39m4\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(list_pkv), \u001b[39m4\u001b[39m)\n\u001b[1;32m     75\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:219\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    217\u001b[0m     output_names \u001b[39m=\u001b[39m [output\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_meta]\n\u001b[1;32m    218\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sess\u001b[39m.\u001b[39;49mrun(output_names, input_feed, run_options)\n\u001b[1;32m    220\u001b[0m \u001b[39mexcept\u001b[39;00m C\u001b[39m.\u001b[39mEPFail \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    221\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mRuntimeException\u001b[0m: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Encountered unknown exception in Run()"
     ]
    }
   ],
   "source": [
    "tokens = model.generate(input_ids=token['input_ids'],\n",
    "               attention_mask=token['attention_mask'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
